{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handout_week_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYd2Ul6g9zwaH5Sb1BnCbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balintnegyesi/Neural_Networks_in_Finance_2022_UU/blob/main/handout_week_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy example\n",
        "\n",
        "In this homework exercise we give two classification toy examples. The first implements the classification problem of the Iris dataset explained in the lectures, in the second the students are expected to implement a neural network classification algorithm for the MNIST dataset, labeling handwritten digits."
      ],
      "metadata": {
        "id": "OQxLMcHCRRN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iris dataset"
      ],
      "metadata": {
        "id": "uhunjhNkSElO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See lecture 1 for details.\n",
        "We use PyTorch. Additionally, we rely on the following libraries."
      ],
      "metadata": {
        "id": "0QDLGXP5SMYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for loading the dataset only\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "zjLQsaVKSYPA"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load the dataset."
      ],
      "metadata": {
        "id": "BSU83ZVrUMur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()  # dictionary\n",
        "X = dataset['data']\n",
        "y = dataset['target']\n",
        "names = dataset['target_names']\n",
        "feature_names = dataset['feature_names']"
      ],
      "metadata": {
        "id": "-LfbfJlLXNUe"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo92rPHeU7DS",
        "outputId": "c5730479-3193-4b00-9b9a-3f9560a61c42"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vaw8cM0VDPC",
        "outputId": "8db4b54b-fbe4-47f1-f79a-02c98875b949"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150,)"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNQjkYCoYaBa",
        "outputId": "80cb8797-4347-41f1-fa5b-1722a78e699a"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unW_6wUVYa6S",
        "outputId": "11ba8e51-5bae-4914-a5ff-b3df65045fac"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "5a7C5YrJT-zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset in a disjoint partition of train and test sets. We take a random 4:1 ratio between the sample sizes."
      ],
      "metadata": {
        "id": "E3ffpJ0EUaXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = X.shape[0]\n",
        "random_idx_ord = np.random.permutation(M)\n",
        "train_indices = random_idx_ord[0: int(0.8 * M)]\n",
        "test_indices = random_idx_ord[int(0.8 * M): ]\n",
        "\n",
        "X_train = X[train_indices, :]\n",
        "y_train = y[train_indices]\n",
        "\n",
        "X_test = X[test_indices, :]\n",
        "y_test = y[test_indices]\n",
        "\n",
        "print('Number of training samples: %d'%X_train.shape[0])\n",
        "print('Number of test samples: %d'%X_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtVIb0u3T-Ht",
        "outputId": "d2855f94-bb53-47f0-85f9-c5db00bba5c8"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 120\n",
            "Number of test samples: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the logistic regression and optimize logistic regression model."
      ],
      "metadata": {
        "id": "lo3uZHEQVM6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(penalty='none', fit_intercept=True,\n",
        "                           solver='newton-cg', max_iter=10000, verbose=0)\n",
        "clf = model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3-2LQu9OT98K"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal coefficients"
      ],
      "metadata": {
        "id": "FEoFZOz8bvJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta = clf.coef_\n",
        "beta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "258sgHHcbtaD",
        "outputId": "102c303a-9187-4f70-c245-fbc777ac3b2b"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 170.25753155,  388.20101541, -425.01100611, -225.49130057],\n",
              "       [ -37.14254913,   21.65627414,  -65.28954899, -646.30275743],\n",
              "       [-133.11498212, -409.85728926,  490.30055509,  871.79405798]])"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting predictions are as follows."
      ],
      "metadata": {
        "id": "-3K7AsN7bOZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Estimated test labels: \", y_pred)\n",
        "print(\"True test labels:      \", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5zqswFPZzD6",
        "outputId": "9f9b48cc-a7c2-4a82-ac14-8db926ea9016"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated test labels:  [2 2 2 1 2 1 2 1 1 2 0 1 2 2 1 1 0 2 1 2 0 0 2 1 2 2 1 2 1 1]\n",
            "True test labels:       [2 2 2 1 2 2 2 1 1 2 0 0 2 2 1 1 0 2 1 2 0 1 2 1 2 2 1 2 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy over the test sample"
      ],
      "metadata": {
        "id": "aTvg30hWbgKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJH40PGgbjqs",
        "outputId": "3b165211-6c7a-4205-cbab-c945086ae286"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network regressions"
      ],
      "metadata": {
        "id": "bhRl_J5WnUDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we define fully-connected, feedforward neural network with $L$ hidden layers, $p_n$ neurons in each layer and a given activation $\\varphi: R\\to R$"
      ],
      "metadata": {
        "id": "otpF8zdAYwUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_dtype(torch.float64)  # mismatch between numpy and pytorch default"
      ],
      "metadata": {
        "id": "V_7IzfH8eVtE"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shallow neural nets"
      ],
      "metadata": {
        "id": "BQDv34a6nkZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowNet(nn.Module):\n",
        "    def __init__(self, input_dimension, output_dimension, num_neurons,\n",
        "                 activation, output_activation):\n",
        "        super(ShallowNet, self).__init__()\n",
        "\n",
        "        self.hidden_layer = nn.Linear(input_dimension, num_neurons)\n",
        "        # the corresponding affine transformation of two full-connected dense\n",
        "        # layers\n",
        "        self.output_layer = nn.Linear(num_neurons, output_dimension)\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.output_layer(x)\n",
        "        output = self.output_activation(x)\n",
        "        return output\n",
        "\n",
        "    def train_minibatch(self, x_train, y_train, epochs=200, log_freq=10,\n",
        "                        batch_size=4):\n",
        "      \n",
        "        losses = []\n",
        "\n",
        "        permutation = torch.randperm(x_train.size()[0])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, x_train.size()[0], batch_size):\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                indices = permutation[i: i+batch_size]\n",
        "                batch_x, batch_y = x_train[indices], y_train[indices]\n",
        "\n",
        "                y_pred = self.forward(batch_x)\n",
        "                current_loss = self.loss(y_pred, batch_y)\n",
        "    \n",
        "                self.optimizer.zero_grad()\n",
        "                current_loss.backward()\n",
        "                self.optimizer.step()\n",
        "            losses.append(current_loss.item())\n",
        "            if epoch % log_freq == 0:\n",
        "              print(f'epoch: {epoch:2}  training loss: {current_loss.item():10.8f}')\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train(self, x_train, y_train, epochs=10**5, log_freq=1000):\n",
        "        losses = []\n",
        "        \n",
        "        for i in range(epochs):\n",
        "          y_pred = self.forward(x_train)\n",
        "          current_loss = self.loss(y_pred, y_train)\n",
        "          losses.append(current_loss.item())\n",
        "          if i % log_freq == 0:\n",
        "            print(f'epoch: {i:2}  training loss: {current_loss.item():10.8f}')\n",
        "    \n",
        "          self.optimizer.zero_grad()\n",
        "          current_loss.backward()\n",
        "          self.optimizer.step()\n",
        "        \n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "Ee7Ffgy7Xh_L"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For reasons which become clear in Lecture 3, PyTorch does not \"like\" numpy inputs. Therefore, we convert the corresponding data sets to so called \"tensors\", and pass those onto the model."
      ],
      "metadata": {
        "id": "5Dj7DaminqMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShallowNet(len(feature_names), len(names), 10, torch.relu, torch.nn.Softmax(dim=-1))\n",
        "\n",
        "y_train_pt = np.zeros(shape=[len(y_train), len(names)])\n",
        "for m in range(len(y_train)):\n",
        "  idx = y_train[m]\n",
        "  y_train_pt[m, idx] = 1\n",
        "\n",
        "X_train_pt = torch.from_numpy(X_train).type(torch.DoubleTensor)\n",
        "y_train_pt = torch.from_numpy(y_train_pt).type(torch.DoubleTensor)\n",
        "\n",
        "X_test_pt = torch.from_numpy(X_test).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "model(X_train_pt).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW0pwP6RfPcx",
        "outputId": "31f0db61-7fc7-4fcb-d985-a1dc63da116a"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([120, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(X_train_pt, y_train_pt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMI09nzbgctc",
        "outputId": "675a7e01-2534-4d50-9805-0360c7117805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  training loss: 1.12720807\n",
            "epoch: 1000  training loss: 0.58485265\n",
            "epoch: 2000  training loss: 0.57604015\n",
            "epoch: 3000  training loss: 0.57288305\n",
            "epoch: 4000  training loss: 0.57106022\n",
            "epoch: 5000  training loss: 0.56981584\n",
            "epoch: 6000  training loss: 0.56887727\n",
            "epoch: 7000  training loss: 0.56811944\n",
            "epoch: 8000  training loss: 0.56747939\n",
            "epoch: 9000  training loss: 0.56692290\n",
            "epoch: 10000  training loss: 0.56642996\n",
            "epoch: 11000  training loss: 0.56598788\n",
            "epoch: 12000  training loss: 0.56558808\n",
            "epoch: 13000  training loss: 0.56522444\n",
            "epoch: 14000  training loss: 0.56489234\n",
            "epoch: 15000  training loss: 0.56458813\n",
            "epoch: 16000  training loss: 0.56430881\n",
            "epoch: 17000  training loss: 0.56405185\n",
            "epoch: 18000  training loss: 0.56381507\n",
            "epoch: 19000  training loss: 0.56359656\n",
            "epoch: 20000  training loss: 0.56339461\n",
            "epoch: 21000  training loss: 0.56320772\n",
            "epoch: 22000  training loss: 0.56303454\n",
            "epoch: 23000  training loss: 0.56287387\n",
            "epoch: 24000  training loss: 0.56272462\n",
            "epoch: 25000  training loss: 0.56258579\n",
            "epoch: 26000  training loss: 0.56245649\n",
            "epoch: 27000  training loss: 0.56233592\n",
            "epoch: 28000  training loss: 0.56222337\n",
            "epoch: 29000  training loss: 0.56211815\n",
            "epoch: 30000  training loss: 0.56201969\n",
            "epoch: 31000  training loss: 0.56192745\n",
            "epoch: 32000  training loss: 0.56184092\n",
            "epoch: 33000  training loss: 0.56175967\n",
            "epoch: 34000  training loss: 0.56168329\n",
            "epoch: 35000  training loss: 0.56161141\n",
            "epoch: 36000  training loss: 0.56154369\n",
            "epoch: 37000  training loss: 0.56147982\n",
            "epoch: 38000  training loss: 0.56141953\n",
            "epoch: 39000  training loss: 0.56136257\n",
            "epoch: 40000  training loss: 0.56130875\n",
            "epoch: 41000  training loss: 0.56125780\n",
            "epoch: 42000  training loss: 0.56120951\n",
            "epoch: 43000  training loss: 0.56116373\n",
            "epoch: 44000  training loss: 0.56112064\n",
            "epoch: 45000  training loss: 0.56107969\n",
            "epoch: 46000  training loss: 0.56104076\n",
            "epoch: 47000  training loss: 0.56100371\n",
            "epoch: 48000  training loss: 0.56096841\n",
            "epoch: 49000  training loss: 0.56093476\n",
            "epoch: 50000  training loss: 0.56090265\n",
            "epoch: 51000  training loss: 0.56087199\n",
            "epoch: 52000  training loss: 0.56084269\n",
            "epoch: 53000  training loss: 0.56081467\n",
            "epoch: 54000  training loss: 0.56078786\n",
            "epoch: 55000  training loss: 0.56076219\n",
            "epoch: 56000  training loss: 0.56073759\n",
            "epoch: 57000  training loss: 0.56071400\n",
            "epoch: 58000  training loss: 0.56069138\n",
            "epoch: 59000  training loss: 0.56066967\n",
            "epoch: 60000  training loss: 0.56064882\n",
            "epoch: 61000  training loss: 0.56062878\n",
            "epoch: 62000  training loss: 0.56060950\n",
            "epoch: 63000  training loss: 0.56059097\n",
            "epoch: 64000  training loss: 0.56057311\n",
            "epoch: 65000  training loss: 0.56055592\n",
            "epoch: 66000  training loss: 0.56053935\n",
            "epoch: 67000  training loss: 0.56052337\n",
            "epoch: 68000  training loss: 0.56050795\n",
            "epoch: 69000  training loss: 0.56049306\n",
            "epoch: 70000  training loss: 0.56047869\n",
            "epoch: 71000  training loss: 0.56046480\n",
            "epoch: 72000  training loss: 0.56045138\n",
            "epoch: 73000  training loss: 0.56043839\n",
            "epoch: 74000  training loss: 0.56042584\n",
            "epoch: 75000  training loss: 0.56041369\n",
            "epoch: 76000  training loss: 0.56040193\n",
            "epoch: 77000  training loss: 0.56039054\n",
            "epoch: 78000  training loss: 0.56037950\n",
            "epoch: 79000  training loss: 0.56036880\n",
            "epoch: 80000  training loss: 0.56035843\n",
            "epoch: 81000  training loss: 0.56034836\n",
            "epoch: 82000  training loss: 0.56033859\n",
            "epoch: 83000  training loss: 0.56032911\n",
            "epoch: 84000  training loss: 0.56031990\n",
            "epoch: 85000  training loss: 0.56031095\n",
            "epoch: 86000  training loss: 0.56030225\n",
            "epoch: 87000  training loss: 0.56029380\n",
            "epoch: 88000  training loss: 0.56028558\n",
            "epoch: 89000  training loss: 0.56027759\n",
            "epoch: 90000  training loss: 0.56026981\n",
            "epoch: 91000  training loss: 0.56026224\n",
            "epoch: 92000  training loss: 0.56025487\n",
            "epoch: 93000  training loss: 0.56024769\n",
            "epoch: 94000  training loss: 0.56024070\n",
            "epoch: 95000  training loss: 0.56023388\n",
            "epoch: 96000  training loss: 0.56022724\n",
            "epoch: 97000  training loss: 0.56022077\n",
            "epoch: 98000  training loss: 0.56021445\n",
            "epoch: 99000  training loss: 0.56020830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
        "\n",
        "num_misclassified = np.sum(y_pred != y_test)\n",
        "\n",
        "print(\"Estimated test labels: \", y_pred)\n",
        "print(\"True test labels:      \", y_test)\n",
        "\n",
        "print(\"Test classification accuracy: %.2f\"%(1 - num_misclassified / len(y_test)))"
      ],
      "metadata": {
        "id": "wNM23-t7hEUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep neural networks"
      ],
      "metadata": {
        "id": "m4NeKVoxn7JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNet(nn.Module):\n",
        "    def __init__(self, input_dimension, output_dimension, num_neurons,\n",
        "                 activation, output_activation):\n",
        "        super(DeepNet, self).__init__()\n",
        "\n",
        "        self.hidden_layers = [nn.Linear(input_dimension, num_neurons[0])]\n",
        "        for idx, width in enumerate(num_neurons):\n",
        "            if idx < (len(num_neurons) - 1):\n",
        "              self.hidden_layers.append(nn.Linear(width, num_neurons[idx + 1]))\n",
        "\n",
        "        self.output_layer = nn.Linear(num_neurons[-1], output_dimension)\n",
        "\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for idx, layer in enumerate(self.hidden_layers):\n",
        "          x = layer(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        output = self.output_activation(x)\n",
        "        return output\n",
        "\n",
        "    def train_minibatch(self, x_train, y_train, epochs=200, log_freq=10,\n",
        "                        batch_size=4):\n",
        "      \n",
        "        losses = []\n",
        "\n",
        "        permutation = torch.randperm(x_train.size()[0])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, x_train.size()[0], batch_size):\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                indices = permutation[i: i+batch_size]\n",
        "                batch_x, batch_y = x_train[indices], y_train[indices]\n",
        "\n",
        "                y_pred = self.forward(batch_x)\n",
        "                current_loss = self.loss(y_pred, batch_y)\n",
        "    \n",
        "                self.optimizer.zero_grad()\n",
        "                current_loss.backward()\n",
        "                self.optimizer.step()\n",
        "            losses.append(current_loss.item())\n",
        "            if epoch % log_freq == 0:\n",
        "              print(f'epoch: {epoch:2}  training loss: {current_loss.item():10.8f}')\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train(self, x_train, y_train, epochs=10**5, log_freq=1000):\n",
        "        losses = []\n",
        "        \n",
        "        for i in range(epochs):\n",
        "          y_pred = self.forward(x_train)\n",
        "          current_loss = self.loss(y_pred, y_train)\n",
        "          losses.append(current_loss.item())\n",
        "          if i % log_freq == 0:\n",
        "            print(f'epoch: {i:2}  training loss: {current_loss.item():10.8f}')\n",
        "    \n",
        "          self.optimizer.zero_grad()\n",
        "          current_loss.backward()\n",
        "          self.optimizer.step()\n",
        "        \n",
        "        return None"
      ],
      "metadata": {
        "id": "OEoYwphCJa4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose network architecture (width, activation functions)"
      ],
      "metadata": {
        "id": "HtFHsZKVpqB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_model = DeepNet(len(feature_names), len(names), [10, 10], torch.relu, torch.nn.Softmax(dim=-1))\n",
        "\n",
        "deep_model.train(X_train_pt, y_train_pt)"
      ],
      "metadata": {
        "id": "3axa_33Oekkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = deep_model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
        "\n",
        "num_misclassified = np.sum(y_pred != y_test)\n",
        "\n",
        "print(\"Estimated test labels: \", y_pred)\n",
        "print(\"True test labels:      \", y_test)\n",
        "\n",
        "print(\"Test classification accuracy: %.2f\"%(1 - num_misclassified / len(y_test)))"
      ],
      "metadata": {
        "id": "8s9x8_sj97Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "jsGwTX3OnKQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_logit = clf.predict(X_test)\n",
        "y_pred_shallow = model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
        "y_pred_deep = deep_model(X_test_pt).detach().numpy().argmax(axis=-1)"
      ],
      "metadata": {
        "id": "aQLfbatinLPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_logit = np.zeros(shape=[len(names), len(names)])\n",
        "confusion_shallow = np.zeros(shape=[len(names), len(names)])\n",
        "confusion_deep = np.zeros(shape=[len(names), len(names)])"
      ],
      "metadata": {
        "id": "L_mAazJknHFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for true_class in range(3):\n",
        "    for predicted_class in range(3):\n",
        "      confusion_logit[true_class, predicted_class] = np.sum(y_pred_logit[y_test == true_class] == predicted_class)\n",
        "      confusion_shallow[true_class, predicted_class] = np.sum(y_pred_shallow[y_test == true_class] == predicted_class)\n",
        "      confusion_deep[true_class, predicted_class] = np.sum(y_pred_deep[y_test == true_class] == predicted_class)\n",
        "\n"
      ],
      "metadata": {
        "id": "ywsfM0yinHJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_logit"
      ],
      "metadata": {
        "id": "7zHBli_Jo9et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_shallow"
      ],
      "metadata": {
        "id": "R-0cngDnpaAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_deep"
      ],
      "metadata": {
        "id": "NWGij0a4pgkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the corresponding accuracies"
      ],
      "metadata": {
        "id": "lNUT0gCnqs1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_logit = np.sum(np.diag(confusion_logit)) / len(y_test)\n",
        "acc_shallow = np.sum(np.diag(confusion_shallow)) / len(y_test)\n",
        "acc_deep = np.sum(np.diag(confusion_deep)) / len(y_test)\n",
        "\n",
        "\n",
        "print('Accuracy logistic regression: %.2f'%acc_logit)\n",
        "print('Accuracy shallow neural network: %.2f'%acc_shallow)\n",
        "print('Accuracy deep neural network: %.2f'%acc_deep)"
      ],
      "metadata": {
        "id": "Li3hu3rspxp7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
