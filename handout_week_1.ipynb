{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQxLMcHCRRN-"
   },
   "source": [
    "# Toy example\n",
    "\n",
    "In this homework exercise we give two classification toy examples. The first implements the classification problem of the Iris dataset explained in the lectures, in the second the students are expected to implement a neural network classification algorithm for the MNIST dataset, labeling handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhunjhNkSElO"
   },
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QDLGXP5SMYE"
   },
   "source": [
    "See lecture 1 for details.\n",
    "We use PyTorch. Additionally, we rely on the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjLQsaVKSYPA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for loading the dataset only\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSU83ZVrUMur"
   },
   "source": [
    "We first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LfbfJlLXNUe"
   },
   "outputs": [],
   "source": [
    "dataset = load_iris()  # dictionary\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "names = dataset['target_names']\n",
    "feature_names = dataset['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo92rPHeU7DS",
    "outputId": "3f9a7680-471b-4a41-ec8d-30200f6a4a56"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vaw8cM0VDPC",
    "outputId": "78dedaa0-0e1c-47d4-b50e-e060d6f82ac4"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNQjkYCoYaBa",
    "outputId": "12d680a3-e278-4298-8728-15cce936652d"
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unW_6wUVYa6S",
    "outputId": "a8d49183-fe40-46da-a5f4-610a6f403a08"
   },
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a7C5YrJT-zE"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3ffpJ0EUaXs"
   },
   "source": [
    "Split the dataset in a disjoint partition of train and test sets. We take a random 4:1 ratio between the sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtVIb0u3T-Ht",
    "outputId": "cee9b729-d76c-4f4e-de65-98b437306853"
   },
   "outputs": [],
   "source": [
    "M = X.shape[0]\n",
    "random_idx_ord = np.random.permutation(M)\n",
    "train_indices = random_idx_ord[0: int(0.8 * M)]\n",
    "test_indices = random_idx_ord[int(0.8 * M): ]\n",
    "\n",
    "X_train = X[train_indices, :]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_test = X[test_indices, :]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print('Number of training samples: %d'%X_train.shape[0])\n",
    "print('Number of test samples: %d'%X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo3uZHEQVM6n"
   },
   "source": [
    "Build the logistic regression and optimize logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-2LQu9OT98K"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None, fit_intercept=True,\n",
    "                           solver='newton-cg', max_iter=10000, verbose=0)\n",
    "clf = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEoFZOz8bvJ2"
   },
   "source": [
    "The optimal coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "258sgHHcbtaD",
    "outputId": "8451fb64-f1ad-4795-c2d3-c85984689682"
   },
   "outputs": [],
   "source": [
    "beta = clf.coef_\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3K7AsN7bOZC"
   },
   "source": [
    "The resulting predictions are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5zqswFPZzD6",
    "outputId": "4e32a5f7-30f7-4ee7-b52b-887a98abe1f4"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Estimated test labels: \", y_pred)\n",
    "print(\"True test labels:      \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTvg30hWbgKe"
   },
   "source": [
    "The accuracy over the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJH40PGgbjqs",
    "outputId": "18e793d6-3961-4aec-a3e5-4b71460538ff"
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhRl_J5WnUDW"
   },
   "source": [
    "# Neural network regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otpF8zdAYwUJ"
   },
   "source": [
    "At this point, we define fully-connected, feedforward neural network with $L$ hidden layers, $p_n$ neurons in each layer and a given activation $\\varphi: R\\to R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_7IzfH8eVtE"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)  # mismatch between numpy and pytorch default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQDv34a6nkZR"
   },
   "source": [
    "## Shallow neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee7Ffgy7Xh_L"
   },
   "outputs": [],
   "source": [
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension, num_neurons,\n",
    "                 activation, output_activation):\n",
    "        super(ShallowNet, self).__init__()\n",
    "\n",
    "        self.hidden_layer = nn.Linear(input_dimension, num_neurons)\n",
    "        # the corresponding affine transformation of two full-connected dense\n",
    "        # layers\n",
    "        self.output_layer = nn.Linear(num_neurons, output_dimension)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        output = self.output_activation(x)\n",
    "        return output\n",
    "\n",
    "    def train_minibatch(self, x_train, y_train, epochs=200, log_freq=10,\n",
    "                        batch_size=4):\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        permutation = torch.randperm(x_train.size()[0])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, x_train.size()[0], batch_size):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                indices = permutation[i: i+batch_size]\n",
    "                batch_x, batch_y = x_train[indices], y_train[indices]\n",
    "\n",
    "                y_pred = self.forward(batch_x)\n",
    "                current_loss = self.loss(y_pred, batch_y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                current_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            losses.append(current_loss.item())\n",
    "            if epoch % log_freq == 0:\n",
    "              print(f'epoch: {epoch:2}  training loss: {current_loss.item():10.8f}')\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=10**5, log_freq=1000):\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "          y_pred = self.forward(x_train)\n",
    "          current_loss = self.loss(y_pred, y_train)\n",
    "          losses.append(current_loss.item())\n",
    "          if i % log_freq == 0:\n",
    "            print(f'epoch: {i:2}  training loss: {current_loss.item():10.8f}')\n",
    "\n",
    "          self.optimizer.zero_grad()\n",
    "          current_loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dj7DaminqMl"
   },
   "source": [
    "For reasons which become clear in Lecture 3, PyTorch does not \"like\" numpy inputs. Therefore, we convert the corresponding data sets to so called \"tensors\", and pass those onto the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW0pwP6RfPcx",
    "outputId": "7c61fe2f-3958-49ff-b607-53f22237c8af"
   },
   "outputs": [],
   "source": [
    "model = ShallowNet(len(feature_names), len(names), 10, torch.relu, torch.nn.Softmax(dim=-1))\n",
    "\n",
    "y_train_pt = np.zeros(shape=[len(y_train), len(names)])\n",
    "for m in range(len(y_train)):\n",
    "  idx = y_train[m]\n",
    "  y_train_pt[m, idx] = 1\n",
    "\n",
    "X_train_pt = torch.from_numpy(X_train).type(torch.DoubleTensor)\n",
    "y_train_pt = torch.from_numpy(y_train_pt).type(torch.DoubleTensor)\n",
    "\n",
    "X_test_pt = torch.from_numpy(X_test).type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "model(X_train_pt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMI09nzbgctc",
    "outputId": "39c22eb7-d014-4a21-c1bc-f4797bf0cfa2"
   },
   "outputs": [],
   "source": [
    "model.train(X_train_pt, y_train_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNM23-t7hEUA",
    "outputId": "3e5f9316-8c9d-47b0-f17e-c6fb0eea3cf3"
   },
   "outputs": [],
   "source": [
    "y_pred = model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
    "\n",
    "num_misclassified = np.sum(y_pred != y_test)\n",
    "\n",
    "print(\"Estimated test labels: \", y_pred)\n",
    "print(\"True test labels:      \", y_test)\n",
    "\n",
    "print(\"Test classification accuracy: %.2f\"%(1 - num_misclassified / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4NeKVoxn7JP"
   },
   "source": [
    "## Deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEoYwphCJa4F"
   },
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension, num_neurons,\n",
    "                 activation, output_activation):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.hidden_layers = [nn.Linear(input_dimension, num_neurons[0])]\n",
    "        layers = [input_dimension] + num_neurons\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers) - 1)])\n",
    "\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # In PyTorch you need to use the construction of nn.ModuleList() by which your variables are properly appended to self.parameters().\n",
    "        # See also the discussion:\n",
    "        # https://discuss.pytorch.org/t/function-class-for-generating-variable-depth-networks/59223\n",
    "        # For those of you who end up using Tensorflow in the future,\n",
    "        # the way I did it in the handout is perfectly fine there, Tensorflow\n",
    "        # does not require custom list structures.\n",
    "        # (That's also the reason why I made this mistake in the first place.)\n",
    "        #\n",
    "        #\n",
    "        # for idx, width in enumerate(num_neurons):\n",
    "        #     if idx < (len(num_neurons) - 1):\n",
    "        #       self.hidden_layers.append(nn.Linear(width, num_neurons[idx + 1]))\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        self.output_layer = nn.Linear(num_neurons[-1], output_dimension)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.hidden_layers):\n",
    "          x = layer(x)\n",
    "          x = self.activation(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        output = self.output_activation(x)\n",
    "        return output\n",
    "\n",
    "    def train_minibatch(self, x_train, y_train, epochs=200, log_freq=10,\n",
    "                        batch_size=4):\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        permutation = torch.randperm(x_train.size()[0])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, x_train.size()[0], batch_size):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                indices = permutation[i: i+batch_size]\n",
    "                batch_x, batch_y = x_train[indices], y_train[indices]\n",
    "\n",
    "                y_pred = self.forward(batch_x)\n",
    "                current_loss = self.loss(y_pred, batch_y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                current_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            losses.append(current_loss.item())\n",
    "            if epoch % log_freq == 0:\n",
    "              print(f'epoch: {epoch:2}  training loss: {current_loss.item():10.8f}')\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=10**5, log_freq=1000):\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "          y_pred = self.forward(x_train)\n",
    "          current_loss = self.loss(y_pred, y_train)\n",
    "          losses.append(current_loss.item())\n",
    "          if i % log_freq == 0:\n",
    "            print(f'epoch: {i:2}  training loss: {current_loss.item():10.8f}')\n",
    "\n",
    "          self.optimizer.zero_grad()\n",
    "          current_loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtFHsZKVpqB8"
   },
   "source": [
    "Choose network architecture (width, activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3axa_33Oekkl",
    "outputId": "afbab3e7-b329-413e-fc32-d960869ee710"
   },
   "outputs": [],
   "source": [
    "deep_model = DeepNet(len(feature_names), len(names), [10, 10], torch.relu, torch.nn.Softmax(dim=-1))\n",
    "\n",
    "deep_model.train(X_train_pt, y_train_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s9x8_sj97Ed",
    "outputId": "90bac44f-85fd-4f23-fe43-55797ddfe26e"
   },
   "outputs": [],
   "source": [
    "    y_pred = deep_model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
    "\n",
    "num_misclassified = np.sum(y_pred != y_test)\n",
    "\n",
    "print(\"Estimated test labels: \", y_pred)\n",
    "print(\"True test labels:      \", y_test)\n",
    "\n",
    "print(\"Test classification accuracy: %.2f\"%(1 - num_misclassified / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsGwTX3OnKQO"
   },
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQLfbatinLPv"
   },
   "outputs": [],
   "source": [
    "y_pred_logit = clf.predict(X_test)\n",
    "y_pred_shallow = model(X_test_pt).detach().numpy().argmax(axis=-1)\n",
    "y_pred_deep = deep_model(X_test_pt).detach().numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_mAazJknHFC"
   },
   "outputs": [],
   "source": [
    "confusion_logit = np.zeros(shape=[len(names), len(names)])\n",
    "confusion_shallow = np.zeros(shape=[len(names), len(names)])\n",
    "confusion_deep = np.zeros(shape=[len(names), len(names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywsfM0yinHJ-"
   },
   "outputs": [],
   "source": [
    "for true_class in range(3):\n",
    "    for predicted_class in range(3):\n",
    "      confusion_logit[true_class, predicted_class] = np.sum(y_pred_logit[y_test == true_class] == predicted_class)\n",
    "      confusion_shallow[true_class, predicted_class] = np.sum(y_pred_shallow[y_test == true_class] == predicted_class)\n",
    "      confusion_deep[true_class, predicted_class] = np.sum(y_pred_deep[y_test == true_class] == predicted_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zHBli_Jo9et"
   },
   "outputs": [],
   "source": [
    "confusion_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-0cngDnpaAz"
   },
   "outputs": [],
   "source": [
    "confusion_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWGij0a4pgkE"
   },
   "outputs": [],
   "source": [
    "confusion_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNUT0gCnqs1u"
   },
   "source": [
    "And the corresponding accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Li3hu3rspxp7"
   },
   "outputs": [],
   "source": [
    "acc_logit = np.sum(np.diag(confusion_logit)) / len(y_test)\n",
    "acc_shallow = np.sum(np.diag(confusion_shallow)) / len(y_test)\n",
    "acc_deep = np.sum(np.diag(confusion_deep)) / len(y_test)\n",
    "\n",
    "\n",
    "print('Accuracy logistic regression: %.2f'%acc_logit)\n",
    "print('Accuracy shallow neural network: %.2f'%acc_shallow)\n",
    "print('Accuracy deep neural network: %.2f'%acc_deep)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
